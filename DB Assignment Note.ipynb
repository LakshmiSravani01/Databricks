{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbe6611f-891d-4280-8b7e-b28ea15bba9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "def init():\n",
    "    spark.sql(\"create database if not exists bronze\")\n",
    "    spark.sql(\"create database if not exists silver\")\n",
    "    spark.sql(\"create database if not exists gold\")\n",
    "\n",
    "    config = [\n",
    "        {\n",
    "            \"file_path\": \"/FileStore/shared_uploads/lakshmisravani208@gmail.com/media_customer_reviews.parquet\",\n",
    "            \"tbl_name\": \"media_customer_reviews\",\n",
    "            \"primary_keys\": [\"new_id\"],\n",
    "            \"type\": \"dim\"\n",
    "        },\n",
    "        {\n",
    "            \"file_path\": \"/FileStore/shared_uploads/lakshmisravani208@gmail.com/media_gold_reviews_chunked.parquet\",\n",
    "            \"tbl_name\": \"media_gold_reviews_chunked\",\n",
    "            \"primary_keys\": [\"franchiseID\", \"chunk_id\"],\n",
    "            \"type\": \"fact\"\n",
    "        },\n",
    "        {\n",
    "            \"file_path\": \"/FileStore/shared_uploads/lakshmisravani208@gmail.com/sales_customers.parquet\",\n",
    "            \"tbl_name\": \"sales_customers\",\n",
    "            \"primary_keys\": [\"customerID\"],\n",
    "            \"type\": \"dim\"\n",
    "        },\n",
    "        {\n",
    "            \"file_path\": \"/FileStore/shared_uploads/lakshmisravani208@gmail.com/sales_franchises.parquet\",\n",
    "            \"tbl_name\": \"sales_franchises\",\n",
    "            \"primary_keys\": [\"franchiseID\"],\n",
    "            \"type\": \"dim\"\n",
    "        },\n",
    "        {\n",
    "            \"file_path\": \"/FileStore/shared_uploads/lakshmisravani208@gmail.com/sales_suppliers.parquet\",\n",
    "            \"tbl_name\": \"sales_suppliers\",\n",
    "            \"primary_keys\": [\"supplierID\"],\n",
    "            \"type\": \"dim\"\n",
    "        },\n",
    "        {\n",
    "            \"file_path\": \"/FileStore/shared_uploads/lakshmisravani208@gmail.com/sales_transactions.parquet\",\n",
    "            \"tbl_name\": \"sales_transactions\",\n",
    "            \"primary_keys\": [\"transactionID\"],\n",
    "            \"type\": \"fact\"\n",
    "        }\n",
    "    ]\n",
    "    print(\"Initialized DB & created config\")\n",
    "    return config\n",
    "\n",
    "\n",
    "def load_to_bronze_table(db: str, parquet_path: str, table_name: str):\n",
    "\n",
    "    table_exists = spark.catalog.tableExists(f\"{db}.{table_name}\")\n",
    "    df = spark.read.parquet(parquet_path)\n",
    "    df = df.withColumn('inserted_at', current_timestamp())\n",
    "    if table_exists:\n",
    "        df.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{db}.{table_name}\")\n",
    "    else:\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{db}.{table_name}\")\n",
    "        \n",
    "def load_with_autoloader(self, db: str, parquet_path: str, table_name: str):\n",
    "        try:\n",
    "            table_exists = self.spark.catalog.tableExists(f\"{db}.{table_name}\")\n",
    "            df = self.spark.readStream \\\n",
    "                .format(\"parquet\") \\\n",
    "                .option(\"cloudFiles.format\", \"parquet\") \\\n",
    "                .option(\"cloudFiles.schemaLocation\", f\"/tmp/schema/{db}/{table_name}\")\\\n",
    "                .load(parquet_path) \\\n",
    "                .withColumn(\"inserted_at\", current_timestamp())\n",
    "\n",
    "            checkpoint_path = f\"/tmp/checkpoints/{db}_{table_name}\"\n",
    "\n",
    "            query = df.writeStream \\\n",
    "                .format(\"delta\") \\\n",
    "                .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .trigger(once=True) \\\n",
    "                .toTable(f\"{db}.{table_name}\")\n",
    "\n",
    "            query.awaitTermination()\n",
    "            logging.info(f\"Auto Loader loaded data into bronze table: {db}.{table_name}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Auto Loader failed for bronze table {db}.{table_name}: {e}\")\n",
    "\n",
    "def cleanup(table_name):\n",
    "    \n",
    "    table_path = f\"/user/hive/warehouse/{table_name}\"\n",
    "    try:\n",
    "        dbutils.fs.rm(f\"dbfs:{table_path}\", True)\n",
    "    except:\n",
    "        print(f\"{table} files Not found\")\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE {table}\")\n",
    "    except:\n",
    "        print(f\"{table} Not found\")\n",
    "\n",
    "def scd1_merge_bronze_to_silver_sql(src_db: str, dest_db: str, tbl_name: str, primary_keys: list):\n",
    "\n",
    "    df_bronze = spark.table(f\"{src_db}.{tbl_name}\").withColumn(\"updated_at\", current_timestamp()).drop(\"inserted_at\")\n",
    "    df_bronze = df_bronze.dropDuplicates(subset=primary_keys)\n",
    "    df_bronze.createOrReplaceTempView(\"bronze_tmp\")\n",
    "    table_exist = spark.catalog.tableExists(f\"{dest_db}.{tbl_name}\")\n",
    "\n",
    "    if not table_exist:\n",
    "        df_bronze.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{dest_db}.{tbl_name}\")\n",
    "        print(f\"Created new silver table: {tbl_name}\")\n",
    "        return\n",
    "    \n",
    "    pk_conditions = \" AND \".join([f\"target.{col} = source.{col}\" for col in primary_keys])\n",
    "    merge_sql = f\"\"\"\n",
    "        MERGE INTO {dest_db}.{tbl_name} AS target\n",
    "        USING bronze_tmp AS source\n",
    "        ON {pk_conditions}\n",
    "        WHEN MATCHED THEN\n",
    "          UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN\n",
    "          INSERT *\n",
    "    \"\"\"\n",
    "    spark.sql(merge_sql)\n",
    "    print(f\"SCD Type 1 merge completed: {src_db}.{tbl_name} -> {dest_db}.{tbl_name}\")\n",
    "\n",
    "def load_to_gold_table(src_db: str,dest_db: str, tbl_name: str, tbl_type: str):\n",
    "\n",
    "    df = spark.table(f\"{src_db}.{tbl_name}\")\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{dest_db}.{tbl_type}_{tbl_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed910929-2308-4762-846e-0bc515a464f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for entry in config:\n",
    "    load_with_autoloader(\"bronze\", entry[\"file_path\"], entry[\"tbl_name\"])\n",
    "    scd1_merge_bronze_to_silver_sql(\"bronze\", \"silver\", entry[\"tbl_name\"], entry[\"primary_keys\"])\n",
    "    load_to_gold_table(\"silver\", \"gold\", entry[\"tbl_name\"], entry[\"type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41ecb3c1-73f2-4d72-bb7c-88fc82a7f49d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Most sold products to identify the top-selling items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9a41875-3105-4a32-9e58-49b700aaeaa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n|             product|no_of_products_sold|\n+--------------------+-------------------+\n|  Golden Gate Ginger|               3865|\n|     Outback Oatmeal|               3733|\n|Austin Almond Bis...|               3716|\n|       Tokyo Tidbits|               3662|\n|         Pearly Pies|               3595|\n|       Orchard Oasis|               3586|\n+--------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"gold.fact_sales_transactions\")\n",
    "result_df = df.groupBy(\"product\") \\\n",
    "              .sum(\"quantity\") \\\n",
    "              .withColumnRenamed(\"sum(quantity)\", \"no_of_products_sold\") \\\n",
    "              .orderBy(\"no_of_products_sold\", ascending=False)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7ea7459-8ba3-4d99-b207-3da5fb78010e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Suppliers provide ingredients to the most franchises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc19396e-638a-49ad-8e12-8923faeec6be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n|supplierID|no_of_franchises|\n+----------+----------------+\n|   4000022|               1|\n|   4000034|               1|\n|   4000021|               1|\n|   4000005|               1|\n|   4000003|               1|\n|   4000044|               1|\n|   4000004|               1|\n|   4000037|               1|\n|   4000039|               1|\n|   4000047|               1|\n|   4000045|               1|\n|   4000031|               1|\n|   4000009|               1|\n|   4000015|               1|\n|   4000019|               1|\n|   4000013|               1|\n|   4000026|               1|\n|   4000018|               1|\n|   4000028|               1|\n|   4000032|               1|\n+----------+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"gold.dim_sales_franchises\")\n",
    "result_df = df.groupBy(\"supplierID\") \\\n",
    "              .count() \\\n",
    "              .withColumnRenamed(\"count\", \"no_of_franchises\") \\\n",
    "              .orderBy(\"no_of_franchises\", ascending=False)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05dc6371-7ac6-4576-a743-b574f1c8f841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Total sales per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ef348f-b708-4725-a1cd-6db2685cf70c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n|sales_month|sales_amount|\n+-----------+------------+\n|          5|     66471.0|\n+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month, sum, col, to_date, expr\n",
    "\n",
    "df = spark.table(\"gold.fact_sales_transactions\")\n",
    "\n",
    "df = df.withColumn(\"totalPrice\", col(\"totalPrice\").cast(\"double\"))\n",
    "df = df.withColumn(\"sales_month\", expr(\"extract(month from dateTime)\"))\n",
    "\n",
    "result_df = df.groupBy(\"sales_month\") \\\n",
    "              .agg(sum(\"totalPrice\").alias(\"sales_amount\")) \\\n",
    "              .orderBy(\"sales_amount\", ascending=False)\n",
    "result_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks Assignment",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}